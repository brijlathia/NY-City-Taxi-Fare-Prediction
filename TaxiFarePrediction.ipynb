{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact number of rows: 55423857\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from sqlalchemy import create_engine\n",
    "import tensorflow as tf\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.client import device_lib\n",
    "import h5py\n",
    "from tensorflow import keras\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Justin Chow Taxi Fare Preiction Model\n",
    "# https://www.kaggle.com/c/new-york-city-taxi-fare-prediction\n",
    "\n",
    "#print (\"TensorFlow version: \" + tf.__version__)\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "TRAIN_PATH = '/Users/mchow3/Desktop/csula/4661/Project/train.csv'\n",
    "TEST_PATH = '/Users/mchow3/Desktop/csula/4661/Project/test.csv'\n",
    "\n",
    "with open(TRAIN_PATH) as file:\n",
    "    n_rows = len(file.readlines())\n",
    "\n",
    "print (f'Exact number of rows: {n_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 8 columns):\n",
      "key                  5 non-null object\n",
      "fare_amount          5 non-null float64\n",
      "pickup_datetime      5 non-null object\n",
      "pickup_longitude     5 non-null float64\n",
      "pickup_latitude      5 non-null float64\n",
      "dropoff_longitude    5 non-null float64\n",
      "dropoff_latitude     5 non-null float64\n",
      "passenger_count      5 non-null int64\n",
      "dtypes: float64(5), int64(1), object(2)\n",
      "memory usage: 400.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_tmp = pd.read_csv(TRAIN_PATH, nrows=5)\n",
    "df_tmp.head()\n",
    "df_tmp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintypes = {'fare_amount': 'float32',\n",
    "              'pickup_datetime': 'str', \n",
    "              'pickup_longitude': 'float32',\n",
    "              'pickup_latitude': 'float32',\n",
    "              'dropoff_longitude': 'float32',\n",
    "              'dropoff_latitude': 'float32',\n",
    "              'passenger_count': 'uint8'}\n",
    "\n",
    "cols = list(traintypes.keys())\n",
    "\n",
    "chunksize = 5_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [01:48,  7.09s/it]\n"
     ]
    }
   ],
   "source": [
    "df_list = [] # list to hold the batch dataframe\n",
    "\n",
    "for df_chunk in tqdm(pd.read_csv(TRAIN_PATH, usecols=cols, dtype=traintypes, chunksize=chunksize)):\n",
    "     \n",
    "    # Neat trick from https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost\n",
    "    # Using parse_dates would be much slower!\n",
    "    df_chunk['pickup_datetime'] = df_chunk['pickup_datetime'].str.slice(0, 16)\n",
    "    df_chunk['pickup_datetime'] = pd.to_datetime(df_chunk['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
    "    \n",
    "    # Can process each chunk of dataframe here\n",
    "    # clean_data(), feature_engineer(),fit()\n",
    "    \n",
    "    # Alternatively, append the chunk to list and merge all\n",
    "    df_list.append(df_chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1 ms\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55423856 entries, 0 to 55423855\n",
      "Data columns (total 7 columns):\n",
      "fare_amount          float32\n",
      "pickup_datetime      datetime64[ns, UTC]\n",
      "pickup_longitude     float32\n",
      "pickup_latitude      float32\n",
      "dropoff_longitude    float32\n",
      "dropoff_latitude     float32\n",
      "passenger_count      uint8\n",
      "dtypes: datetime64[ns, UTC](1), float32(5), uint8(1)\n",
      "memory usage: 1.5 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2009-06-15 17:26:00+00:00</td>\n",
       "      <td>-73.844315</td>\n",
       "      <td>40.721317</td>\n",
       "      <td>-73.841614</td>\n",
       "      <td>40.712276</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.9</td>\n",
       "      <td>2010-01-05 16:52:00+00:00</td>\n",
       "      <td>-74.016045</td>\n",
       "      <td>40.711304</td>\n",
       "      <td>-73.979271</td>\n",
       "      <td>40.782005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2011-08-18 00:35:00+00:00</td>\n",
       "      <td>-73.982735</td>\n",
       "      <td>40.761269</td>\n",
       "      <td>-73.991241</td>\n",
       "      <td>40.750561</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2012-04-21 04:30:00+00:00</td>\n",
       "      <td>-73.987129</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991570</td>\n",
       "      <td>40.758091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.3</td>\n",
       "      <td>2010-03-09 07:51:00+00:00</td>\n",
       "      <td>-73.968094</td>\n",
       "      <td>40.768009</td>\n",
       "      <td>-73.956657</td>\n",
       "      <td>40.783764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fare_amount           pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0          4.5 2009-06-15 17:26:00+00:00        -73.844315        40.721317   \n",
       "1         16.9 2010-01-05 16:52:00+00:00        -74.016045        40.711304   \n",
       "2          5.7 2011-08-18 00:35:00+00:00        -73.982735        40.761269   \n",
       "3          7.7 2012-04-21 04:30:00+00:00        -73.987129        40.733143   \n",
       "4          5.3 2010-03-09 07:51:00+00:00        -73.968094        40.768009   \n",
       "\n",
       "   dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0         -73.841614         40.712276                1  \n",
       "1         -73.979271         40.782005                1  \n",
       "2         -73.991241         40.750561                2  \n",
       "3         -73.991570         40.758091                1  \n",
       "4         -73.956657         40.783764                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55423851</th>\n",
       "      <td>14.0</td>\n",
       "      <td>2014-03-15 03:28:00+00:00</td>\n",
       "      <td>-74.005272</td>\n",
       "      <td>40.740028</td>\n",
       "      <td>-73.963280</td>\n",
       "      <td>40.762554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423852</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2009-03-24 20:46:00+00:00</td>\n",
       "      <td>-73.957787</td>\n",
       "      <td>40.765530</td>\n",
       "      <td>-73.951637</td>\n",
       "      <td>40.773960</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423853</th>\n",
       "      <td>14.1</td>\n",
       "      <td>2011-04-02 22:04:00+00:00</td>\n",
       "      <td>-73.970505</td>\n",
       "      <td>40.752323</td>\n",
       "      <td>-73.960541</td>\n",
       "      <td>40.797340</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423854</th>\n",
       "      <td>28.9</td>\n",
       "      <td>2011-10-26 05:57:00+00:00</td>\n",
       "      <td>-73.980904</td>\n",
       "      <td>40.764629</td>\n",
       "      <td>-73.870605</td>\n",
       "      <td>40.773964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55423855</th>\n",
       "      <td>7.5</td>\n",
       "      <td>2014-12-12 11:33:00+00:00</td>\n",
       "      <td>-73.969719</td>\n",
       "      <td>40.797668</td>\n",
       "      <td>-73.970886</td>\n",
       "      <td>40.783314</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fare_amount           pickup_datetime  pickup_longitude  \\\n",
       "55423851         14.0 2014-03-15 03:28:00+00:00        -74.005272   \n",
       "55423852          4.2 2009-03-24 20:46:00+00:00        -73.957787   \n",
       "55423853         14.1 2011-04-02 22:04:00+00:00        -73.970505   \n",
       "55423854         28.9 2011-10-26 05:57:00+00:00        -73.980904   \n",
       "55423855          7.5 2014-12-12 11:33:00+00:00        -73.969719   \n",
       "\n",
       "          pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "55423851        40.740028         -73.963280         40.762554   \n",
       "55423852        40.765530         -73.951637         40.773960   \n",
       "55423853        40.752323         -73.960541         40.797340   \n",
       "55423854        40.764629         -73.870605         40.773964   \n",
       "55423855        40.797668         -73.970886         40.783314   \n",
       "\n",
       "          passenger_count  \n",
       "55423851                1  \n",
       "55423852                1  \n",
       "55423853                1  \n",
       "55423854                1  \n",
       "55423855                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# Merge all dataframes into one dataframe\n",
    "train_df = pd.concat(df_list)\n",
    "\n",
    "# Delete the dataframe list to release memory\n",
    "del df_list\n",
    "\n",
    "# See what we have loaded\n",
    "train_df.info()\n",
    "\n",
    "display(train_df.head())\n",
    "display(train_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-953b098b7ff0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Save into feather format, about 1.5Gb.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_feather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nyc_taxi_data_raw.feather'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10_000_000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Save into feather format, about 1.5Gb.\n",
    "train_df.to_feather('nyc_taxi_data_raw.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "key                   object\n",
       "fare_amount          float64\n",
       "pickup_datetime       object\n",
       "pickup_longitude     float64\n",
       "pickup_latitude      float64\n",
       "dropoff_longitude    float64\n",
       "dropoff_latitude     float64\n",
       "passenger_count        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.kaggle.com/dster/nyc-taxi-fare-starter-kernel-simple-linear-model\n",
    "\n",
    "#train_df_new = pd.read_feather('nyc_taxi_data_raw.feather')\n",
    "#train_df_new.info()\n",
    "\n",
    "train_df =  pd.read_csv(TRAIN_PATH, nrows = 10_000_000)\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key                    0\n",
      "fare_amount            0\n",
      "pickup_datetime        0\n",
      "pickup_longitude       0\n",
      "pickup_latitude        0\n",
      "dropoff_longitude     69\n",
      "dropoff_latitude      69\n",
      "passenger_count        0\n",
      "abs_diff_longitude    69\n",
      "abs_diff_latitude     69\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Given a dataframe, add two new features 'abs_diff_longitude' and\n",
    "# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n",
    "# the pickup location to the dropoff location.\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "add_travel_vector_features(train_df)\n",
    "\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old size: 10000000\n",
      "New size: 9999931\n"
     ]
    }
   ],
   "source": [
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df.dropna(how = 'any', axis = 'rows')\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = train_df.iloc[:2000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old size: 9999931\n",
      "New size: 9979187\n"
     ]
    }
   ],
   "source": [
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44247132, 6)\n",
      "(9979187,)\n"
     ]
    }
   ],
   "source": [
    "# Construct and return an Nx3 input matrix for our linear model\n",
    "# using the travel vector, plus a 1.0 for a constant bias term.\n",
    "def get_input_matrix(df):\n",
    "    return np.column_stack((df.abs_diff_longitude, df.abs_diff_latitude, np.ones(len(df))))\n",
    "\n",
    "X_train = get_input_matrix(train_df)\n",
    "y_train = np.array(train_df['fare_amount'])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147.16176525  76.95503724   6.39545245]\n"
     ]
    }
   ],
   "source": [
    "# The lstsq function returns several things, and we only care about the actual weight vector w.\n",
    "(w, _, _, _) = np.linalg.lstsq(X_train, y_train, rcond = None)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147.16176525  76.95503724   6.39545245]\n"
     ]
    }
   ],
   "source": [
    "w_OLS = np.matmul(np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), X_train.T), y_train)\n",
    "print(w_OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "key                   object\n",
       "pickup_datetime       object\n",
       "pickup_longitude     float64\n",
       "pickup_latitude      float64\n",
       "dropoff_longitude    float64\n",
       "dropoff_latitude     float64\n",
       "passenger_count        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(TEST_PATH)\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '.pytest_cache', 'GCP-Coupons-Instructions.rtf', 'name-of-file.h5', 'new-york-city-taxi-fare-prediction.zip', 'nyc_taxi_data_raw.feather', 'sample_submission.csv', 'submission.csv', 'TaxiFarePrediction.ipynb', 'test.csv', 'train.csv', 'train_h5py']\n"
     ]
    }
   ],
   "source": [
    "# Reuse the above helper functions to add our features and generate the input matrix.\n",
    "add_travel_vector_features(test_df)\n",
    "X_test = get_input_matrix(test_df)\n",
    "\n",
    "# Predict fare_amount on the test set using our model (w) trained on the training set.\n",
    "y_test_predictions = np.matmul(X_test, w).round(decimals = 2)\n",
    "\n",
    "# Write the predictions to a CSV file which we can submit to the competition.\n",
    "submission = pd.DataFrame(\n",
    "    {'key': test_df.key, 'fare_amount': y_test_predictions},\n",
    "    columns = ['key', 'fare_amount'])\n",
    "submission.to_csv('submission.csv', index = False)\n",
    "\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (11084772,) (9914,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-64657d22d8a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_test_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(left, right)\u001b[0m\n\u001b[0;32m   1581\u001b[0m             \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1583\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_na_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1584\u001b[0m         return construct_result(left, result,\n\u001b[0;32m   1585\u001b[0m                                 index=left.index, name=res_name, dtype=None)\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36msafe_na_op\u001b[1;34m(lvalues, rvalues)\u001b[0m\n\u001b[0;32m   1527\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1528\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1529\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mna_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1530\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mna_op\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpressions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1505\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1506\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmasked_arith_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(op, op_str, a, b, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[0muse_numexpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_numexpr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0m_bool_arith_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b, **eval_kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (11084772,) (9914,) "
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(np.mean((y_test_predictions-targets)**2))\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mchow3\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyarrow\\pandas_compat.py:752: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels, = index.labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55423856 entries, 0 to 55423855\n",
      "Data columns (total 7 columns):\n",
      "fare_amount          float32\n",
      "pickup_datetime      datetime64[ns, UTC]\n",
      "pickup_longitude     float32\n",
      "pickup_latitude      float32\n",
      "dropoff_longitude    float32\n",
      "dropoff_latitude     float32\n",
      "passenger_count      uint8\n",
      "dtypes: datetime64[ns, UTC](1), float32(5), uint8(1)\n",
      "memory usage: 1.5 GB\n"
     ]
    }
   ],
   "source": [
    "train_df_new = pd.read_feather('nyc_taxi_data_raw.feather')\n",
    "\n",
    "train_df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old size: 55423480\n",
      "New size: 55423480\n",
      "Old size: 55423480\n",
      "New size: 55308916\n",
      "(55308916, 3)\n",
      "   abs_diff_longitude  abs_diff_latitude  passenger_count\n",
      "0            0.002701           0.009041                1\n",
      "1            0.036774           0.070702                1\n",
      "2            0.008507           0.010708                2\n",
      "3            0.004440           0.024948                1\n",
      "4            0.011436           0.015755                1\n",
      "(55308916,)\n",
      "0     4.5\n",
      "1    16.9\n",
      "2     5.7\n",
      "3     7.7\n",
      "4     5.3\n",
      "Name: fare_amount, dtype: float32\n",
      "Wall time: 8.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "add_travel_vector_features(train_df_new)\n",
    "\n",
    "print('Old size: %d' % len(train_df_new))\n",
    "train_df_new = train_df_new.dropna(how = 'any', axis = 'rows')\n",
    "print('New size: %d' % len(train_df_new))\n",
    "\n",
    "print('Old size: %d' % len(train_df_new))\n",
    "train_df_new = train_df_new[(train_df_new.abs_diff_longitude < 5.0) & (train_df_new.abs_diff_latitude < 5.0)]\n",
    "print('New size: %d' % len(train_df_new))\n",
    "\n",
    "#feature_cols = ['pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude', 'dropoff_latitude', 'passenger_count']\n",
    "feature_cols = ['abs_diff_longitude', 'abs_diff_latitude', 'passenger_count']\n",
    "\n",
    "x = train_df_new[feature_cols]\n",
    "\n",
    "y = train_df_new['fare_amount']\n",
    "\n",
    "print(x.shape)\n",
    "print(x.head())\n",
    "print(y.shape)\n",
    "print(y.head())\n",
    "\n",
    "#print(train_df_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44247132, 3)\n",
      "(44247132,)\n",
      "(11061784, 3)\n",
      "(11061784,)\n",
      "Wall time: 7.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.45 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# In the following line, \"my_linreg\" is instantiated as an \"object\" of LinearRegression \"class\". \n",
    "my_linreg = LinearRegression()\n",
    "\n",
    "# fitting the model to the training data:\n",
    "my_linreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intercept: \n",
      "6.336703\n",
      "\n",
      "Coef_: \n",
      "[1.4627786e+02 7.6414520e+01 5.6549072e-02]\n",
      "\n",
      "y_prediction: \n",
      "[ 9.879013  8.384809  8.954946 ... 23.866524 14.568426 11.79248 ]\n",
      "\n",
      "RMSE: \n",
      "6.746997\n"
     ]
    }
   ],
   "source": [
    "# printing Theta0 using attribute \"intercept_\":\n",
    "print(\"\\nIntercept: \")\n",
    "print(my_linreg.intercept_)\n",
    "\n",
    "# printing [Theta1, Theta2, Theta3] using attribute \"coef_\":\n",
    "print(\"\\nCoef_: \")\n",
    "print(my_linreg.coef_)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_prediction = my_linreg.predict(x_test)\n",
    "\n",
    "print(\"\\ny_prediction: \")\n",
    "print(y_prediction)\n",
    "\n",
    "# Calculating \"Mean Square Error\" (MSE):\n",
    "mse = metrics.mean_squared_error(y_test, y_prediction)\n",
    "\n",
    "# Using numpy sqrt function to take the square root and calculate \"Root Mean Square Error\" (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"\\nRMSE: \")\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "#train_generator = datagen.flow(x_train, y_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected flatten_13_input to have 3 dimensions, but got array with shape (44339084, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2472\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    563\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    566\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected flatten_13_input to have 3 dimensions, but got array with shape (44339084, 6)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(6, 6)),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "#model.evaluate(x_test, y_test, verbose=2)\n",
    "               \n",
    "\n",
    "#EPOCHS = 100\n",
    "#BS = 32\n",
    "\n",
    "#model.fit_generator(generator=training_generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None,\n",
    "#                    validation_data=(x_test, y_test), validation_steps=None, validation_freq=1,\n",
    "#                    class_weight=None, max_queue_size=10, workers=12, use_multiprocessing=True,\n",
    "#                    shuffle=True, initial_epoch=100)\n",
    "\n",
    "\n",
    "#H = model.fit_generator(aug.flow(x_train, y_train, batch_size=BS),\n",
    "#validation_data=(x_test, y_test), steps_per_epoch=len(x_train) // BS,\n",
    "#epochs=EPOCHS)\n",
    "\n",
    "\n",
    "#model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "#model.train_on_batch(x, y, sample_weight=None, class_weight=None, reset_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
